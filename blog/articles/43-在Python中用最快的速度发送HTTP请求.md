---
title: "åœ¨ Python ä¸­ç”¨æœ€å¿«çš„é€Ÿåº¦å‘é€ HTTP è¯·æ±‚"
date: 2021-08-25
tags: [Python,HTTP]
categories: [ğŸŒ ç¿»è¯‘æ ¡å¯¹]
---

ä½¿ç”¨ `requests` åŒ…å¯ä»¥è½»æ¾å‘é€å•ä¸ª HTTP è¯·æ±‚ã€‚ä½†å¦‚æœæˆ‘æƒ³è¦å¼‚æ­¥åœ°å‘é€ä¸Šç™¾ä¸ªç”šè‡³ä¸Šç™¾ä¸‡ä¸ª HTTP è¯·æ±‚å‘¢ï¼Ÿè¿™ç¯‡æ–‡ç« æ˜¯ä¸€ç¯‡æ¢ç´¢ç¬”è®°ï¼Œæ—¨åœ¨æ‰¾åˆ°å‘é€å¤šä¸ª HTTP è¯·æ±‚çš„æœ€å¿«æ–¹å¼ã€‚
<!-- more -->
ä»£ç åœ¨äº‘ç«¯çš„å¸¦æœ‰ Python 3.7 çš„ Linux(Ubuntu) VM ä¸»æœºä¸­è¿è¡Œã€‚æ‰€æœ‰éƒ½ä»£ç å­˜æ”¾åœ¨ Gist ä¸­ï¼Œéƒ½å¯ä»¥å¤åˆ¶å’Œè¿è¡Œã€‚

## æ–¹æ³• #1 : ä½¿ç”¨åŒæ­¥

è¿™æ˜¯æœ€ç®€å•æ˜“æ‡‚çš„æ–¹å¼ï¼Œä½†ä¹Ÿæ˜¯æœ€æ…¢çš„æ–¹å¼ã€‚æˆ‘é€šè¿‡è¿™ä¸ªç¥å¥‡çš„ Python åˆ—è¡¨æ“ä½œç¬¦ä¸ºæµ‹è¯•ä¼ªé€ äº† 100 ä¸ªé“¾æ¥ï¼š

```py
url_list = ["https://www.google.com/","https://www.bing.com"]*50
```

ä»£ç :

```Python
import requests
import time

def download_link(url:str) -> None:
    result = requests.get(url).content
    print(f'Read {len(result)} from {url}')
def download_all(urls:list) -> None:
    for url in urls:
        download_link(url)
        
url_list = ["https://www.google.com/","https://www.bing.com"]*50
start = time.time()
download_all(url_list)
end = time.time()
print(f'download {len(url_list)} links in {end - start} seconds')
```

å®ƒç”¨äº†åç§’é’Ÿæ¥å®Œæˆä¸‹è½½ 100 ä¸ªé“¾æ¥ã€‚

```
...
download 100 links in 9.438416004180908 seconds
```

ä½œä¸ºä¸€ä¸ªåŒæ­¥çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸ªä»£ç ä»æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ `Session` å¯¹è±¡æ¥æé«˜é€Ÿåº¦ã€‚`Session` å¯¹è±¡å°†ä½¿ç”¨ `urllib3` çš„è¿æ¥æ± ï¼Œè¿™æ„å‘³ç€å¯¹äºå¯¹åŒä¸€ä¸»æœºçš„é‡å¤è¯·æ±‚ï¼Œå°†é‡æ–°ä½¿ç”¨ `Session` å¯¹è±¡çš„åº•å±‚ TCP è¿æ¥ï¼Œä»è€Œè·å¾—æ€§èƒ½æå‡ã€‚

> å› æ­¤ï¼Œå¦‚æœæ‚¨å‘åŒä¸€ä¸»æœºå‘å‡ºå¤šä¸ªè¯·æ±‚ï¼Œåˆ™åº•å±‚ TCP è¿æ¥å°†è¢«é‡ç”¨ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—æå‡ â€”â€” [Session å¯¹è±¡](https://docs.python-requests.org/en/master/user/advanced/)

ä¸ºäº†ç¡®ä¿è¯·æ±‚å¯¹è±¡æ— è®ºæˆåŠŸä¸å¦éƒ½é€€å‡ºï¼Œæˆ‘å°†ä½¿ç”¨ `with` è¯­å¥ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚Python ä¸­çš„ `with` å…³é”®è¯åªæ˜¯æ›¿æ¢ `tryâ€¦â€¦ finallyâ€¦â€¦` çš„ä¸€ä¸ªå¹²å‡€çš„è§£å†³æ–¹æ¡ˆã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å°†ä»£ç æ”¹æˆè¿™æ ·å¯ä»¥èŠ‚çœå¤šå°‘ç§’ï¼š

```Python
import requests
from requests.sessions import Session
import time

url_list = ["https://www.google.com/","https://www.bing.com"]*50

def download_link(url:str,session:Session):
    with session.get(url) as response:
        result = response.content
        print(f'Read {len(result)} from {url}')

def download_all(urls:list):
    with requests.Session() as session:
        for url in urls:
            download_link(url,session=session)

start = time.time()
download_all(url_list)
end = time.time()
print(f'download {len(url_list)} links in {end - start} seconds')
```

çœ‹èµ·æ¥æ€§èƒ½çœŸçš„æå‡åˆ°äº† 5.x ç§’ã€‚

```
...
download 100 links in 5.367443561553955 seconds
```

ä½†æ˜¯è¿™æ ·è¿˜æ˜¯å¾ˆæ…¢ï¼Œè®©æˆ‘ä»¬è¯•è¯•å¤šçº¿ç¨‹çš„è§£å†³æ–¹æ¡ˆã€‚

## è§£å†³æ–¹æ¡ˆ#2ï¼šå¤šçº¿ç¨‹æ–¹æ³•

Python çº¿ç¨‹æ˜¯ä¸€ä¸ªå±é™©çš„è¯é¢˜ï¼Œæœ‰æ—¶ï¼Œå¤šçº¿ç¨‹å¯èƒ½ä¼šæ›´æ…¢ï¼æˆ´ç»´Â·æ¯”å…¹åˆ©ï¼ˆDavid Beazleyï¼‰å¸¦æ¥äº†ä¸€åœºç²¾å½©çš„ã€æ¶µç›–äº†è¿™ä¸ªå±é™©çš„è¯é¢˜çš„æ¼”è®²ã€‚YouTube é“¾æ¥åœ¨[è¿™é‡Œ](https://docs.python-requests.org/en/master/user/advanced/)ã€‚

æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»ç„¶ä¼šä½¿ç”¨ Python çº¿ç¨‹æ¥å®Œæˆ HTTP è¯·æ±‚å·¥ä½œã€‚æˆ‘å°†ä½¿ç”¨ä¸€ä¸ªé˜Ÿåˆ—æ¥ä¿å­˜ 100 ä¸ªé“¾æ¥å¹¶åˆ›å»º 10 ä¸ª HTTP å·¥ä½œçº¿ç¨‹æ¥å¼‚æ­¥ä¸‹è½½è¿™ 100 ä¸ªé“¾æ¥ã€‚

![How the multi-thread works](https://cdn-images-1.medium.com/max/2994/1*JfPvverf5eScyBkQQ6NOqw.png)

è¦ä½¿ç”¨ Session å¯¹è±¡ï¼Œä¸º 10 ä¸ªçº¿ç¨‹åˆ›å»º 10 ä¸ª Session å¯¹è±¡æ˜¯ä¸€ç§æµªè´¹ï¼Œæˆ‘åªæƒ³è¦åˆ›å»ºä¸€ä¸ª Session å¯¹è±¡å¹¶åœ¨æ‰€æœ‰ä¸‹è½½å·¥ä½œä¸­é‡ç”¨å®ƒã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œä»£ç å°†åˆ©ç”¨ `threading` åŒ…ä¸­çš„ `local` å¯¹è±¡ï¼Œè¿™æ · 10 ä¸ªçº¿ç¨‹å·¥ä½œå°†å…±äº«ä¸€ä¸ª Session å¯¹è±¡ã€‚

```py
from threading import Thread,local
...
thread_local = local()
...
```

ä»£ç ï¼š

```Python
import requests
from requests.sessions import Session
import time
from threading import Thread,local
from queue import Queue

url_list = ["https://www.google.com/","https://www.bing.com"]*50
q = Queue(maxsize=0)            #Use a queue to store all URLs
for url in url_list:
    q.put(url)
thread_local = local()          #The thread_local will hold a Session object

def get_session() -> Session:
    if not hasattr(thread_local,'session'):
        thread_local.session = requests.Session() # Create a new Session if not exists
    return thread_local.session

def download_link() -> None:
    '''download link worker, get URL from queue until no url left in the queue'''
    session = get_session()
    while True:
        url = q.get()
        with session.get(url) as response:
            print(f'Read {len(response.content)} from {url}')
        q.task_done()          # tell the queue, this url downloading work is done

def download_all(urls) -> None:
    '''Start 10 threads, each thread as a wrapper of downloader'''
    thread_num = 10
    for i in range(thread_num):
        t_worker = Thread(target=download_link)
        t_worker.start()
    q.join()                   # main thread wait until all url finished downloading

print("start work")
start = time.time()
download_all(url_list)
end = time.time()
print(f'download {len(url_list)} links in {end - start} seconds')
```

ç»“æœï¼š

```
...
download 100 links in 1.1333789825439453 seconds
```

ä¸‹è½½çš„é€Ÿåº¦éå¸¸å¿«ï¼æ¯”åŒæ­¥è§£å†³æ–¹æ¡ˆå¿«å¾—å¤šã€‚

## è§£å†³æ–¹æ¡ˆ #3ï¼šé€šè¿‡ ThreadPoolExecutor è¿›è¡Œå¤šçº¿ç¨‹

Python è¿˜æä¾›äº† `ThreadPoolExecutor` æ¥æ‰§è¡Œå¤šçº¿ç¨‹å·¥ä½œï¼Œæˆ‘å¾ˆå–œæ¬¢ `ThreadPoolExecutor`ã€‚

åœ¨ Thread + Queue çš„ç‰ˆæœ¬ä¸­ï¼ŒHTTP è¯·æ±‚å·¥ä½œä¸­æœ‰ä¸€ä¸ª `while True` å¾ªç¯ï¼Œè¿™ä½¿å¾— `worker` å‡½æ•°ä¸ Queue çº ç¼ ä¸æ¸…ï¼Œä»åŒæ­¥ç‰ˆæœ¬å˜æ›´åˆ°å¼‚æ­¥ç‰ˆæœ¬çš„ä»£ç éœ€è¦é¢å¤–çš„æ”¹åŠ¨ã€‚

æœ‰äº† ThreadPoolExecutor åŠå…¶ map å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªä»£ç éå¸¸ç®€æ´çš„å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼Œåªéœ€è¦ä»åŒæ­¥ç‰ˆæœ¬ä¸­è¿›è¡Œå¾ˆå°çš„ä»£ç æ›´æ”¹ã€‚

![How the ThreadPoolExecutor version works](https://cdn-images-1.medium.com/max/2676/1*21PJpOn4vMaFCgJPn1CDbQ.png)

ä»£ç ï¼š

```Python
import requests
from requests.sessions import Session
import time
from concurrent.futures import ThreadPoolExecutor
from threading import Thread,local

url_list = ["https://www.google.com/","https://www.bing.com"]*50
thread_local = local()

def get_session() -> Session:
    if not hasattr(thread_local,'session'):
        thread_local.session = requests.Session()
    return thread_local.session

def download_link(url:str):
    session = get_session()
    with session.get(url) as response:
        print(f'Read {len(response.content)} from {url}')

def download_all(urls:list) -> None:
    with ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(download_link,url_list)

start = time.time()
download_all(url_list)
end = time.time()
print(f'download {len(url_list)} links in {end - start} seconds')
```

æœ€åçš„è¾“å‡ºå’Œçº¿ç¨‹é˜Ÿåˆ—ç‰ˆæœ¬ä¸€æ ·å¿«ï¼š

```
...
download 100 links in 1.0798051357269287 seconds
```

## è§£å†³æ–¹æ¡ˆ #4ï¼šasyncio ä¸ aiohttp

æ¯ä¸ªäººéƒ½è¯´ `asyncio` å°±æ˜¯æœªæ¥ï¼Œè€Œä¸”é€Ÿåº¦å¾ˆå¿«ã€‚æœ‰äº›äººä½¿ç”¨å®ƒ [æ¥ç”¨ Python çš„ `asyncio` å’Œ `aiohttp` åŒ…å‘å‡º 100 ä¸‡æ¬¡ HTTP è¯·æ±‚](https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html)ã€‚å°½ç®¡ `asyncio` éå¸¸å¿«ï¼Œä½†å®ƒ**æ²¡æœ‰**ä½¿ç”¨ Python å¤šçº¿ç¨‹ã€‚

ä½ æ•¢ç›¸ä¿¡å—ï¼Œ`asyncio` åªæœ‰åœ¨ä¸€ä¸ªçº¿ç¨‹ã€ä¸€ä¸ª CPU æ ¸å¿ƒä¸­è¿è¡Œï¼

![asyncio Event Loop](https://cdn-images-1.medium.com/max/3490/1*yByf16mv2X7XaaTX4oQFaA.png)

åœ¨ `asyncio` ä¸­å®ç°çš„äº‹ä»¶å¾ªç¯å‡ ä¹ä¸ Javascript ä¸­ä½¿ç”¨çš„ç›¸åŒã€‚

Asyncio éå¸¸å¿«ï¼Œå‡ ä¹å¯ä»¥å‘æœåŠ¡å™¨å‘é€ä»»æ„æ•°é‡çš„è¯·æ±‚ï¼Œå”¯ä¸€çš„é™åˆ¶æ˜¯æ‚¨çš„è®¾å¤‡å’Œäº’è”ç½‘å¸¦å®½ã€‚

å‘é€è¿‡å¤šçš„ HTTP è¯·æ±‚ä¼šè¡¨ç°å¾—åƒâ€œæ”»å‡»â€ã€‚å½“æ£€æµ‹åˆ°å¤ªå¤šè¯·æ±‚æ—¶ï¼ŒæŸäº›ç½‘ç«™å¯èƒ½ä¼šå°é”æ‚¨çš„ IP åœ°å€ï¼Œç”šè‡³ Google ä¹Ÿä¼šå°é”æ‚¨ã€‚ä¸ºäº†é¿å…è¢«å°é”ï¼Œæˆ‘ä½¿ç”¨äº†ä¸€ä¸ªè‡ªå®šä¹‰ TCP è¿æ¥å™¨å¯¹è±¡ï¼Œè¯¥å¯¹è±¡å°†æœ€å¤§ TCP è¿æ¥æ•°æŒ‡å®šä¸º 10ã€‚ï¼ˆå°†å…¶æ›´æ”¹ä¸º 20 åº”è¯¥ä¹Ÿæ˜¯å®‰å…¨çš„ã€‚ï¼‰

```py
my_conn = aiohttp.TCPConnector(limit=10)
```

è¿™ä¸ªä»£ç éå¸¸ç®€çŸ­ï¼š

```Python
import asyncio
import time 
import aiohttp
from aiohttp.client import ClientSession

async def download_link(url:str,session:ClientSession):
    async with session.get(url) as response:
        result = await response.text()
        print(f'Read {len(result)} from {url}')

async def download_all(urls:list):
    my_conn = aiohttp.TCPConnector(limit=10)
    async with aiohttp.ClientSession(connector=my_conn) as session:
        tasks = []
        for url in urls:
            task = asyncio.ensure_future(download_link(url=url,session=session))
            tasks.append(task)
        await asyncio.gather(*tasks,return_exceptions=True) # the await must be nest inside of the session

url_list = ["https://www.google.com","https://www.bing.com"]*50
print(url_list)
start = time.time()
asyncio.run(download_all(url_list))
end = time.time()
print(f'download {len(url_list)} links in {end - start} seconds')
```

ä¸Šé¢çš„ä»£ç åœ¨ 0.74 ç§’å†…å®Œæˆäº† 100 ä¸ªé“¾æ¥çš„ä¸‹è½½ï¼

```
...
download 100 links in 0.7412574291229248 seconds
```

è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Jupyter Notebook æˆ– IPython ä¸­è¿è¡Œä»£ç ï¼Œè¯·å®‰è£… `nest-asyncio` åŒ…ã€‚ (æ„Ÿè°¢[è¿™ä¸ª StackOverflow é“¾æ¥](https://stackoverflow.com/questions/46827007/runtimeerror-this-event-loop-is-already-running-in-python)ã€‚æ„Ÿè°¢ [Diaf Badreddine](https://stackoverflow.com/users/12371243/diaf-badreddine)ã€‚)

```bash
pip install nest-asyncio
```

å¹¶åœ¨ä»£ç å¼€å¤´æ·»åŠ ä»¥ä¸‹ä¸¤è¡Œä»£ç ï¼š

```py
import nest_asyncio
nest_asyncio.apply()
```

## è§£å†³æ–¹æ¡ˆ#5ï¼šå¦‚æœæ˜¯ Node.js å‘¢ï¼Ÿ

æˆ‘æƒ³çŸ¥é“ï¼Œå¦‚æœæˆ‘åœ¨å…·æœ‰å†…ç½®äº‹ä»¶å¾ªç¯çš„ Node.js ä¸­åšåŒæ ·çš„å·¥ä½œä¼šæ€æ ·ï¼Ÿ

è¿™æ˜¯å®Œæ•´çš„ä»£ç ã€‚

```JavaScript
const requst = require('request')
//build a 100 links array
url_list = []
url_list.push(...Array(50).fill("https://www.google.com"))
url_list.push(...Array(50).fill("https://www.bing.com"))

const batch_num = 10;     // send 10 Http requesta at one time
let batch_index = batch_num;
let resolvehandler = null;
function download_link(url){
    requst({
        url: url,
        timeout:1000
    },function(error,response,body){
        if (body){
            console.log(body.length)
        }
        batch_index = batch_index -1
        if(batch_index==0){
            batch_index = batch_num
            resolvehandler()
        }
    });
}

function download_batch(url_list){
    for(j = 0;j<batch_num;j++){
        download_link(url_list[j])
    }
}

async function download_all(url_list){
    let loop_count = url_list.length/batch_num;
    console.time('test')
    for(var i =0;i<loop_count;i++){
        await new Promise(function(resolve,reject){
            download_batch(url_list.slice(i,i+10))
            resolvehandler = resolve
        })
    }
    console.timeEnd('test')
}

download_all(url_list)
```

è¿™ä¸ªä»£ç éœ€è¦ 1.1 åˆ° 1.5 ç§’ï¼Œæ‚¨å¯ä»¥è¿è¡Œå®ƒåœ¨æ‚¨çš„è®¾å¤‡ä¸­æŸ¥çœ‹ç»“æœã€‚

```
...
test: 1195.290ms
```

Python èµ¢å¾—äº†è¿™åœºé€Ÿåº¦çš„æ¸¸æˆï¼

ï¼ˆçœ‹èµ·æ¥ Node.js çš„ request åŒ…å·²ç»[è¢«å¼ƒç”¨](https://github.com/request/request/issues/3142)äº†ï¼Œä½†è¿™ä¸ªç¤ºä¾‹åªæ˜¯ä¸ºäº†æµ‹è¯• Node.js çš„äº‹ä»¶å¾ªç¯ä¸ Python çš„äº‹ä»¶å¾ªç¯ç›¸æ¯”å¦‚ä½•æ‰§è¡Œã€‚ï¼‰

---

> * åŸæ–‡åœ°å€ï¼š[Send HTTP Requests As Fast As Possible in Python](https://python.plainenglish.io/send-http-requests-as-fast-as-possible-in-python-304134d46604)
> * åŸæ–‡ä½œè€…ï¼š[Andrew Zhu](https://medium.com/@xhinker)
> * è¯‘æ–‡å‡ºè‡ªï¼š[æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://github.com/xitu/gold-miner)
> * æœ¬æ–‡æ°¸ä¹…é“¾æ¥ï¼š[https://github.com/xitu/gold-miner/blob/master/article/2021/send-http-requests-as-fast-as-possible-in-python.md](https://github.com/xitu/gold-miner/blob/master/article/2021/send-http-requests-as-fast-as-possible-in-python.md)
> * è¯‘è€…ï¼š[ItzMiracleOwO](https://github.com/ItzMiracleOwO)
> * æ ¡å¯¹è€…ï¼š[jaredliw](https://github.com/jaredliw)ã€[KimYangOfCat](https://github.com/KimYangOfCat)